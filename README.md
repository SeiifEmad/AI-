# AI-
Assingment 
 Q1 — Types of Optimizers in Machine Learning

1. **Gradient Descent** — Updates parameters by moving in the direction of the negative gradient.
2. **Stochastic Gradient Descent (SGD)** — Updates parameters for each training example instead of the full dataset.
3. **Mini-Batch Gradient Descent** — Uses small random batches for faster convergence and smoother updates.
4. **Momentum Optimizer** — Accelerates gradient descent by adding a fraction of the previous update to the current one.
5. **Nesterov Accelerated Gradient (NAG)** — Improves momentum by looking ahead before updating the gradient.
6. **Adagrad** — Adapts the learning rate for each parameter based on the frequency of updates.
7. **RMSProp** — Adjusts learning rates using a moving average of squared gradients for stable convergence.
8. **Adam** — Combines Momentum and RMSProp for adaptive and efficient optimization.
9. **Adadelta** — An extension of Adagrad that overcomes its rapid learning rate decay problem.
10. **Adamax** — A variant of Adam that uses the infinity norm, offering better stability for sparse gradients.
